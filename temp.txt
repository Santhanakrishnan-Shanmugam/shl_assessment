from playwright.sync_api import sync_playwright
import csv
import time
from urllib.parse import urljoin

BASE_URL = "https://www.shl.com"
CATALOG_URL = "https://www.shl.com/solutions/products/product-catalog/"
OUTPUT_CSV = "shl_assessments.csv"


def infer_test_type(text):
    t = text.lower()
    if any(k in t for k in ["personality", "behaviour", "behavior", "motivation"]):
        return "P"
    return "K"


def crawl_shl():
    rows = []
    visited = set()

    with sync_playwright() as p:
        context = p.chromium.launch_persistent_context(
            user_data_dir="shl_user_data",
            headless=False,
            viewport={"width": 1280, "height": 800},
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/121.0.0.0 Safari/537.36"
            ),
            args=[
                "--disable-blink-features=AutomationControlled",
            ],
        )

        page = context.new_page()
        page.goto(CATALOG_URL, wait_until="networkidle")

        # Accept cookies (VERY IMPORTANT)
        try:
            page.click("button:has-text('Accept')", timeout=8000)
        except:
            pass

        # Scroll to force hydration
        for _ in range(15):
            page.mouse.wheel(0, 5000)
            time.sleep(1)

        anchors = page.query_selector_all("a")
        print(f"Total <a> tags found: {len(anchors)}")

        links = []
        for a in anchors:
            href = a.get_attribute("href")
            if href and "/products/assessments/" in href:
                links.append(urljoin(BASE_URL, href))

        links = list(set(links))
        print(f"Filtered assessment links: {len(links)}")

        for url in links:
            if url in visited:
                continue
            visited.add(url)

            page.goto(url, wait_until="domcontentloaded")
            time.sleep(1)

            # h1 = page.query_selector("h1")
            # if not h1:
            #     continue

            # name = h1.inner_text().strip()
            h1 = page.query_selector("h1")
            name = h1.inner_text().strip() if h1 and h1.inner_text().strip() else None

            if not name:
                title = page.title()
                name = title.replace(" | SHL", "").strip() if title else None

            if not name:
                continue

            text = page.inner_text("body")

            if "Pre-packaged" in text:
                continue

            test_type = infer_test_type(text)

            rows.append([
                name,
                url,
                "Individual Test Solution",
                test_type
            ])

            print(f"✔ Collected: {name}")

        context.close()

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["name", "url", "category", "test_type"])
        writer.writerows(rows)

    print(f"\n✅ Total Individual Test Solutions collected: {len(rows)}")


if __name__ == "__main__":
    crawl_shl()
    
===========================================================================================
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import time

options = Options()
options.add_argument("--disable-gpu")
options.add_argument("--start-maximized")

driver = webdriver.Chrome(
    service=Service(ChromeDriverManager().install()),
    options=options
)

driver.get("https://www.shl.com/solutions/products/product-catalog/")
time.sleep(8)

# Scroll until no more content loads
last_height = driver.execute_script("return document.body.scrollHeight")
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(3)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# Collect product links
links = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/products/"]')
print(f"Found {len(links)} product links")

data = []
seen = set()

for link in links:
    url = link.get_attribute("href")
    name = link.text.strip()

    if not url or url in seen:
        continue

    seen.add(url)

    data.append({
        "name": name,
        "url": url
    })

driver.quit()

# ✅ SAVE TO CSV
df = pd.DataFrame(data)
df.to_csv("shl_product_links.csv", index=False, encoding="utf-8")

print("CSV saved as shl_product_links.csv")
print(df.head())
=====================================================================================================
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import pandas as pd
import time

options = Options()
options.add_argument("--disable-gpu")
options.add_argument("--start-maximized")

driver = webdriver.Chrome(
    service=Service(ChromeDriverManager().install()),
    options=options
)

# Open the catalog
catalog_url = "https://www.shl.com/solutions/products/product-catalog/"
driver.get(catalog_url)
time.sleep(8)

# Scroll to load all content
last_height = driver.execute_script("return document.body.scrollHeight")
while True:
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(3)
    new_height = driver.execute_script("return document.body.scrollHeight")
    if new_height == last_height:
        break
    last_height = new_height

# Step 1: Collect all hrefs first
links = driver.find_elements(By.CSS_SELECTOR, 'a[href*="/products/"]')
urls = []
for link in links:
    url = link.get_attribute("href")
    if url and url not in urls:
        urls.append(url)

print(f"Collected {len(urls)} URLs")

# Step 2: Visit each URL to enrich data
data = []

for url in urls:
    enriched_name = ""
    description = ""
    
    try:
        driver.get(url)
        time.sleep(3)
        
        # Use <h1> as name if available
        try:
            enriched_name = driver.find_element(By.TAG_NAME, "h1").text.strip()
        except:
            enriched_name = ""
        
        # Grab short description
        try:
            description = driver.find_element(By.TAG_NAME, "main").text.replace("\n", " ")[:300]
        except:
            description = ""
    
    except:
        pass
    
    data.append({
        "name": enriched_name,
        "url": url,
        "description": description
    })

driver.quit()

# Save to CSV
df = pd.DataFrame(data)
df.to_csv("shl_product_links_enriched.csv", index=False, encoding="utf-8")

print(f"CSV saved with {len(df)} rows")
print(df.head())

=========================================================================================================
import pandas as pd
import recommender

# Load labeled train data
TRAIN_XLSX = "../Gen_AI Dataset.xlsx"
df = pd.read_excel(TRAIN_XLSX)
print("Columns in dataset:", df.columns)

# Extract assessment names from URLs
def extract_name(url):
    return url.rstrip("/").split("/")[-1].lower()

df['relevant_assessments'] = df['Assessment_url'].apply(
    lambda x: [extract_name(u) for u in x.split("|")] if isinstance(x, str) else [extract_name(x)]
)

# Metrics
def precision_at_k(preds, truths, k):
    top_k_preds = preds[:k]
    return len(set(top_k_preds) & set(truths)) / k

def recall_at_k(preds, truths, k):
    top_k_preds = preds[:k]
    return len(set(top_k_preds) & set(truths)) / len(truths)

def mrr(preds, truths):
    for idx, pred in enumerate(preds, start=1):
        if pred in truths:
            return 1 / idx
    return 0

# Evaluate
top_k = 5
precision_scores = []
recall_scores = []
mrr_scores = []

for i, row in df.iterrows():
    query = row['Query']
    truths = row['relevant_assessments']

    preds_dicts = recommender.recommend(query, top_k=top_k)
    preds = [extract_name(p['url']) for p in preds_dicts]

    print("Query:", query)
    print("Predictions (top 5):", preds)
    print("Ground truth:", truths)
    print("-"*50)

    precision_scores.append(precision_at_k(preds, truths, top_k))
    recall_scores.append(recall_at_k(preds, truths, top_k))
    mrr_scores.append(mrr(preds, truths))

# Results
print(f"Precision@{top_k}: {sum(precision_scores)/len(precision_scores):.3f}")
print(f"Recall@{top_k}: {sum(recall_scores)/len(recall_scores):.3f}")
print(f"MRR: {sum(mrr_scores)/len(mrr_scores):.3f}")
